{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9Q6UhBeO15hQ"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "# Install required packages\n",
        "%pip install -U langgraph langsmith google-generativeai langchain-google-genai\n",
        "%pip install fastapi nest-asyncio pyngrok uvicorn python-multipart\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from typing import Annotated\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "from typing_extensions import TypedDict\n",
        "from IPython.display import Image, display\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Set up API keys\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDZbz3ts-1661Oi3hkapmFcY952j-ooQT4\"  # Your actual key\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Initialize the LLM\n",
        "gemini = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
        "response = gemini.invoke(\"Hello world!\")\n",
        "print(response)\n",
        "\n",
        "# Define the chatbot graph\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", END)\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "# Display the graph (optional)\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# CLI Chat Interface\n",
        "def stream_graph_updates(user_input: str):\n",
        "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
        "        for value in event.values():\n",
        "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
        "\n",
        "def run_cli_chat():\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"User: \")\n",
        "            if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "                print(\"Goodbye!\")\n",
        "                break\n",
        "\n",
        "            stream_graph_updates(user_input)\n",
        "        except:\n",
        "            user_input = \"What do you know about LangGraph?\"\n",
        "            print(\"User: \" + user_input)\n",
        "            stream_graph_updates(user_input)\n",
        "            break\n",
        "\n",
        "# FastAPI Backend Setup\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "class Message(BaseModel):\n",
        "    role: str\n",
        "    content: str\n",
        "\n",
        "class ChatRequest(BaseModel):\n",
        "    messages: list[Message]\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Allow CORS for your React app\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# print(\"=== YOUR API URL ===\")\n",
        "# print(f\"POST requests to: {ngrok_tunnel.public_url}/chat\")\n",
        "# print(\"===================\")\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat_endpoint(request: ChatRequest):\n",
        "    try:\n",
        "        response = graph.invoke({\"messages\": request.messages})\n",
        "        return {\"message\": response[\"messages\"][-1].content}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "def run_fastapi_server():\n",
        "    ngrok_tunnel = ngrok.connect(8000)\n",
        "    print('Public URL:', ngrok_tunnel.public_url)\n",
        "    nest_asyncio.apply()\n",
        "    uvicorn.run(app, host='0.0.0.0', port=8000)\n",
        "\n",
        "# Uncomment one of these to choose which mode to run in\n",
        "print(\"Ready to run:\")\n",
        "print(\"1. For CLI chat interface, call: run_cli_chat()\")\n",
        "print(\"2. For FastAPI server (React frontend), call: run_fastapi_server()\")\n",
        "\n",
        "# To run the CLI chat:\n",
        "# run_cli_chat()\n",
        "\n",
        "# To run the FastAPI server for React frontend:\n",
        "# run_fastapi_server()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "os.system(\"ngrok config add-authtoken 2unl6n9aIHmHvxyg0ashOPABPRc_6k8vaAjsYZR1puxhCyujE\")\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jJ3wupse6RG",
        "outputId": "bdb93830-3b9f-470e-be5c-138be1210a7d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://67e2-35-245-200-14.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3sX3Cxkpc0dy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}